#  Copyright 2023 Amazon.com and its affiliates; all rights reserved.
#  This file is Amazon Web Services Content and may not be duplicated or distributed without permission.

from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain_community.llms.bedrock import Bedrock
from langchain_core.prompts import PromptTemplate

supported_models = [
    "anthropic.claude-v2:1",
    "anthropic.claude-v1",
    "anthropic.claude-instant-v1",
]


class Chatbot:
    def __init__(
        self,
        model_id="anthropic.claude-v2:1",
        stop=None,
    ):
        if model_id not in supported_models:
            raise ValueError(f"model_id {model_id} is not supported")

        # Please check carefully with updated bedrock documents
        # https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/models
        self.max_token = (
            4096
            if "llama" in model_id
            else 200000
            if "anthropic.claude-v2" in model_id
            else 100000
            if "anthropic.claude-v1" in model_id
            else 4000  # titian
        )

        self.model_kwargs = {
            "temperature": 0.5,
            "top_p": 0.5,
        }

        if "anthropic" in model_id:
            self.model_kwargs["max_tokens_to_sample"] = 1024
            self.model_kwargs["top_k"] = 50
        elif "llama" in model_id:
            self.model_kwargs["max_gen_len"] = 1024

        self.default_stop = ["\n\nHuman:"] if "anthropic" in model_id else []

        self.stop = stop if stop is not None else self.default_stop

        self.engine = Bedrock(
            model_id=model_id, streaming=True, model_kwargs=self.model_kwargs
        )

    def ask_stream(
        self,
        input_text: str,
        conversation_history: ConversationBufferMemory = ConversationBufferMemory(
            ai_prefix="Assistant"
        ),
        verbose: bool = False,
        **kwargs,
    ):
        """Processes a stream of input by invoking the engine.

        Parameters
        ----------
        conversation_history: ConversationBufferMemory
            The conversation history
        input_text: str
            The input prompt or message.
        verbose: boolean
            if the langchain shall show the detailed logging
        kwargs: dict
            Additional keyword arguments to pass to the engine.
            For example, you can pass in temperature to control the model's
            creativity.
        Returns
        -------
        str
            The response generated by the engine.

        Raises
        ------
        Any exceptions raised by the engine.
        """

        temp_stop = kwargs.get("stop", self.stop)

        template = """The following is a friendly conversation between a human and an AI. The AI is able to provide accurate information in a structured way. The AI is able to provide information in a clear and concise manner.If the AI does not know the answer to a question, it truthfully says it does not know.

        Current conversation:
        {history}
        Human: {input}
        Assistant:"""
        PROMPT = PromptTemplate(input_variables=["history", "input"], template=template)

        chain = ConversationChain(
            llm=self.engine,
            memory=conversation_history,
            verbose=verbose,
            prompt=PROMPT,
        )

        return chain.predict(input=input_text, stop=temp_stop)
